{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4df85ea",
   "metadata": {},
   "source": [
    "\n",
    "# <span style = \"color : green\"> DOGS vs CATS </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfba429",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dadf01",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4e4cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img, array_to_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb645337",
   "metadata": {},
   "source": [
    "#### Testing if the Image data generator works fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d652f78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen= ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7248963",
   "metadata": {},
   "source": [
    "##### Explanation\n",
    "\n",
    "rotation_range: The range (in degrees) for random rotations of the input images.\n",
    "\n",
    "width_shift_range: The range (as a fraction of total width) for random horizontal shifts of the input images.\n",
    "\n",
    "height_shift_range: The range (as a fraction of total height) for random vertical shifts of the input images.\n",
    "\n",
    "shear_range: The range (in degrees) for random shearing transformations of the input images.\n",
    "\n",
    "zoom_range: The range for random zooming of the input images.\n",
    "\n",
    "horizontal_flip: A boolean value indicating whether to randomly flip the input images horizontally.\n",
    "\n",
    "fill_mode: The method used for filling in any pixels that may be lost during the above transformations. In this case, 'nearest' is used, which means that the pixel value of the nearest neighboring pixel will be used to fill in any lost pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52602363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: (500, 520)\n",
      "Image format: JPEG\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image_path = \"C:/Users/Administrator/Desktop/DATA-SCIENCE/DL/DogsVsCats/DATASET/cats_or_dogs_1.jpg\"\n",
    "\n",
    "with Image.open(image_path) as image:\n",
    "    print(f\"Image size: {image.size}\")\n",
    "    print(f\"Image format: {image.format}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0929ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_img('C:/Users/Administrator/Desktop/DATA-SCIENCE/DL/DogsVsCats/DATASET/cats_or_dogs_1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebad5225",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.resize((150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bb2789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=img_to_array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5f8b4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 150)\n"
     ]
    }
   ],
   "source": [
    "print(img.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fed3491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=x.reshape((1,)+x.shape) # Numpy array with shape (1,3,150,150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254fb91f",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "This line of code uses the Numpy function reshape() to change the shape of the Numpy array x. The new shape of x will have one extra dimension with a size of 1 compared to the original shape of x.\n",
    "\n",
    "The original shape of x is (3,150,150), which means x has 3 channels and each channel has a 2D image of size 150x150.\n",
    "\n",
    "The new shape of x after reshape() is (1,3,150,150). This means that the array now has an extra dimension at the beginning with a size of 1. The remaining dimensions have the same sizes as the original array. This can be useful when working with deep learning models that expect inputs with a certain number of dimensions, such as convolutional neural networks (CNNs) that expect inputs in the format of (batch_size, channels, height, width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a0abc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for batch in datagen.flow(x,save_to_dir='C:/Users/Administrator/Desktop/DATA-SCIENCE/DL/DogsVsCats/preview',save_prefix='cat',save_format='jpeg'):\n",
    "    i+=1\n",
    "    if i>20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf24c66",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "i = 0: Initializes a counter variable to keep track of the number of augmented images that have been generated.\n",
    "\n",
    "for batch in datagen.flow(x, batch_size=1, save_to_dir='preview', save_prefix='cat', save_format='jpeg'):: This is a for loop that uses the flow method of the datagen object to generate batches of augmented images. The flow method takes as input the x image data array and generates batches of augmented images on-the-fly. The batch_size parameter specifies the number of images to generate in each batch. In this case, we set it to 1. The save_to_dir parameter specifies the directory where the augmented images will be saved. The save_prefix parameter specifies the prefix to be added to the filename of each augmented image. The save_format parameter specifies the file format for the augmented images (in this case, JPEG).\n",
    "\n",
    "i += 1: Increments the counter variable by 1 after each batch of augmented images is generated.\n",
    "\n",
    "if i > 20: break: This statement checks if the number of augmented images generated (i) is greater than 20. If it is, the loop is exited using the break statement. This ensures that only 20 augmented images are generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a6ee32",
   "metadata": {},
   "source": [
    "### Actual model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "698bf4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation,Dropout,Flatten,Dense\n",
    "from tensorflow.keras.layers import ZeroPadding2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59037c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Conv2D(32,(3,3),input_shape=(150,150,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2fa8d3",
   "metadata": {},
   "source": [
    "The size of the filters in a convolutional layer is typically chosen based on the size and complexity of the input image, as well as the desired output. In this case, the input image is 150x150 pixels and has 3 color channels (RGB), so a 3x3 filter size is a common choice for the first convolutional layer.\n",
    "\n",
    "The number of filters in a convolutional layer is often chosen based on the complexity of the problem and the capacity of the model. In this case, 32 filters is a relatively small number and is a common choice for the first convolutional layer in a simple CNN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f132314",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d57941",
   "metadata": {},
   "source": [
    "This line adds a rectified linear unit (ReLU) activation function to the layer. ReLU is commonly used as an activation function in deep learning models because it is simple, fast, and has been shown to work well in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a2ad148",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2343f430",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hidden layer\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd8b7de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  2 Fully-connected layer\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "#1st\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5)) #avoids overfitting\n",
    "\n",
    "#2nd\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0af904d",
   "metadata": {},
   "source": [
    "model.add(Flatten()): This layer flattens the input array to a 1D array. This is necessary because the convolutional and pooling layers output a 3D array, and the next layer in the model is a fully connected layer which expects a 1D array.\n",
    "\n",
    "model.add(Dense(64)): This adds a fully connected layer with 64 neurons.\n",
    "\n",
    "model.add(Activation('relu')): This applies the rectified linear unit (ReLU) activation function to the output of the previous layer. ReLU is a common activation function used in neural networks to introduce nonlinearity.\n",
    "\n",
    "model.add(Dropout(0.5)): This adds a dropout layer with a rate of 0.5, which randomly drops out 50% of the neurons during training. Dropout is a regularization technique used to prevent overfitting.\n",
    "\n",
    "model.add(Dense(1)): This adds a final fully connected layer with a single neuron, which is used for binary classification.\n",
    "\n",
    "model.add(Activation('sigmoid')): This applies the sigmoid activation function to the output of the previous layer. The sigmoid function squashes the output to a range between 0 and 1, which is useful for binary classification.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2ca0fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7385e4b",
   "metadata": {},
   "source": [
    "Let's prepare our data. We will use .flow_from_directory() to generate batches of image data (and their labels) directly from our jpgs in their respective folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2923369",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "\n",
    "#training set\n",
    "train_datagen=ImageDataGenerator(rescale=1./255,\n",
    "                                shear_range=0.2,\n",
    "                                zoom_range=0.2,\n",
    "                                horizontal_flip=True)\n",
    "test_datagen=ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03652767",
   "metadata": {},
   "source": [
    "\n",
    "rescale=1./255: rescales the pixel values of the images to be between 0 and 1, which is a common preprocessing step for image data.\n",
    "\n",
    "shear_range=0.2: randomly applies a shearing transformation to the images, which shifts the positions of pixels along a certain direction.\n",
    "\n",
    "zoom_range=0.2: randomly applies a zooming transformation to the images, which either zooms in or out of the image by a certain factor.\n",
    "\n",
    "horizontal_flip=True: randomly flips the images horizontally, which helps to improve the model's ability to detect objects regardless of their orientation.\n",
    "\n",
    "Together, these transformations create a more diverse and robust training set, which helps to improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ad75193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 557 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'C:/Users/Administrator/Desktop/DATA-SCIENCE/DL/DogsVsCats/train',  # this is the target directory\n",
    "        target_size=(150, 150),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b732b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 140 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator=test_datagen.flow_from_directory(\n",
    "    'C:/Users/Administrator/Desktop/DATA-SCIENCE/DL/DogsVsCats/test',\n",
    "    target_size=(150,150), batch_size=batch_size, class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0867fd3",
   "metadata": {},
   "source": [
    "The rescale parameter normalizes the pixel values of the images to the range [0, 1], the shear_range, zoom_range, and horizontal_flip parameters perform various transformations on the images to augment the dataset and help prevent overfitting.\n",
    "\n",
    "The train_generator variable is created by calling the flow_from_directory method of the ImageDataGenerator instance. This method generates batches of training data by reading images from a directory on the local file system. In this case, the train_generator will read images from the directory specified by the path 'C:/Users/Administrator/Desktop/DATA-SCIENCE/DL/DogsVsCats/train'.\n",
    "\n",
    "The target_size parameter specifies the size to which all images will be resized to before they are fed into the model. Here, all images will be resized to (150, 150) pixels.\n",
    "\n",
    "The batch_size parameter specifies the number of images to include in each batch of training data. This can be adjusted depending on the available memory on the machine running the code.\n",
    "\n",
    "The class_mode parameter specifies the type of label encoding to use for the labels associated with the images. Here, since the problem is a binary classification task (dogs vs cats), the class_mode is set to 'binary'. This means that the labels will be encoded as either 0 or 1.\n",
    "\n",
    "In summary, this code creates an instance of ImageDataGenerator that applies various data augmentation techniques to the images, and then generates batches of training data from a directory of images by calling the flow_from_directory method of the ImageDataGenerator instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fc1f8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_1380\\4255560561.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "34/34 [==============================] - 32s 908ms/step - loss: 0.7142 - accuracy: 0.4935 - val_loss: 0.6936 - val_accuracy: 0.4922\n",
      "Epoch 2/50\n",
      "34/34 [==============================] - 16s 473ms/step - loss: 0.6972 - accuracy: 0.5342 - val_loss: 0.6909 - val_accuracy: 0.5234\n",
      "Epoch 3/50\n",
      "34/34 [==============================] - 16s 472ms/step - loss: 0.6958 - accuracy: 0.5619 - val_loss: 0.6907 - val_accuracy: 0.5312\n",
      "Epoch 4/50\n",
      "34/34 [==============================] - 16s 478ms/step - loss: 0.6946 - accuracy: 0.5656 - val_loss: 0.6909 - val_accuracy: 0.5469\n",
      "Epoch 5/50\n",
      "34/34 [==============================] - 17s 482ms/step - loss: 0.6805 - accuracy: 0.6063 - val_loss: 0.7070 - val_accuracy: 0.5469\n",
      "Epoch 6/50\n",
      "34/34 [==============================] - 16s 465ms/step - loss: 0.6592 - accuracy: 0.5933 - val_loss: 0.6803 - val_accuracy: 0.5625\n",
      "Epoch 7/50\n",
      "34/34 [==============================] - 16s 465ms/step - loss: 0.6584 - accuracy: 0.6155 - val_loss: 0.6741 - val_accuracy: 0.6250\n",
      "Epoch 8/50\n",
      "34/34 [==============================] - 16s 467ms/step - loss: 0.6505 - accuracy: 0.6617 - val_loss: 0.6760 - val_accuracy: 0.6641\n",
      "Epoch 9/50\n",
      "34/34 [==============================] - 17s 497ms/step - loss: 0.6238 - accuracy: 0.6451 - val_loss: 0.7471 - val_accuracy: 0.5547\n",
      "Epoch 10/50\n",
      "34/34 [==============================] - 17s 487ms/step - loss: 0.6277 - accuracy: 0.6913 - val_loss: 0.7218 - val_accuracy: 0.5469\n",
      "Epoch 11/50\n",
      "34/34 [==============================] - 16s 472ms/step - loss: 0.5736 - accuracy: 0.7024 - val_loss: 0.7604 - val_accuracy: 0.5703\n",
      "Epoch 12/50\n",
      "34/34 [==============================] - 16s 466ms/step - loss: 0.5620 - accuracy: 0.6895 - val_loss: 0.8137 - val_accuracy: 0.5781\n",
      "Epoch 13/50\n",
      "34/34 [==============================] - 16s 466ms/step - loss: 0.5939 - accuracy: 0.7098 - val_loss: 0.6594 - val_accuracy: 0.6328\n",
      "Epoch 14/50\n",
      "34/34 [==============================] - 16s 468ms/step - loss: 0.5504 - accuracy: 0.7206 - val_loss: 0.7951 - val_accuracy: 0.6328\n",
      "Epoch 15/50\n",
      "34/34 [==============================] - 16s 466ms/step - loss: 0.5854 - accuracy: 0.7098 - val_loss: 0.9583 - val_accuracy: 0.6094\n",
      "Epoch 16/50\n",
      "34/34 [==============================] - 16s 463ms/step - loss: 0.5691 - accuracy: 0.7301 - val_loss: 0.7570 - val_accuracy: 0.5859\n",
      "Epoch 17/50\n",
      "34/34 [==============================] - 16s 470ms/step - loss: 0.5664 - accuracy: 0.7301 - val_loss: 0.7621 - val_accuracy: 0.6328\n",
      "Epoch 18/50\n",
      "34/34 [==============================] - 16s 470ms/step - loss: 0.5595 - accuracy: 0.7190 - val_loss: 0.7378 - val_accuracy: 0.6406\n",
      "Epoch 19/50\n",
      "34/34 [==============================] - 16s 472ms/step - loss: 0.5316 - accuracy: 0.7338 - val_loss: 0.8951 - val_accuracy: 0.6641\n",
      "Epoch 20/50\n",
      "34/34 [==============================] - 16s 466ms/step - loss: 0.5284 - accuracy: 0.7431 - val_loss: 0.7835 - val_accuracy: 0.6406\n",
      "Epoch 21/50\n",
      "34/34 [==============================] - 16s 465ms/step - loss: 0.5258 - accuracy: 0.7190 - val_loss: 0.8634 - val_accuracy: 0.5859\n",
      "Epoch 22/50\n",
      "34/34 [==============================] - 16s 479ms/step - loss: 0.4827 - accuracy: 0.7671 - val_loss: 0.7411 - val_accuracy: 0.6797\n",
      "Epoch 23/50\n",
      "34/34 [==============================] - 16s 483ms/step - loss: 0.4806 - accuracy: 0.7634 - val_loss: 0.7598 - val_accuracy: 0.6797\n",
      "Epoch 24/50\n",
      "34/34 [==============================] - 16s 475ms/step - loss: 0.4851 - accuracy: 0.7616 - val_loss: 0.7702 - val_accuracy: 0.6641\n",
      "Epoch 25/50\n",
      "34/34 [==============================] - 16s 476ms/step - loss: 0.4649 - accuracy: 0.8059 - val_loss: 0.6913 - val_accuracy: 0.6797\n",
      "Epoch 26/50\n",
      "34/34 [==============================] - 16s 480ms/step - loss: 0.4450 - accuracy: 0.7874 - val_loss: 0.8879 - val_accuracy: 0.6406\n",
      "Epoch 27/50\n",
      "34/34 [==============================] - 16s 465ms/step - loss: 0.4598 - accuracy: 0.7782 - val_loss: 0.9021 - val_accuracy: 0.6953\n",
      "Epoch 28/50\n",
      "34/34 [==============================] - 16s 470ms/step - loss: 0.4463 - accuracy: 0.7856 - val_loss: 0.9390 - val_accuracy: 0.6797\n",
      "Epoch 29/50\n",
      "34/34 [==============================] - 16s 475ms/step - loss: 0.4534 - accuracy: 0.8059 - val_loss: 0.8205 - val_accuracy: 0.6172\n",
      "Epoch 30/50\n",
      "34/34 [==============================] - 16s 469ms/step - loss: 0.4352 - accuracy: 0.8041 - val_loss: 0.8798 - val_accuracy: 0.6875\n",
      "Epoch 31/50\n",
      "34/34 [==============================] - 16s 471ms/step - loss: 0.4358 - accuracy: 0.7948 - val_loss: 0.9037 - val_accuracy: 0.6484\n",
      "Epoch 32/50\n",
      "34/34 [==============================] - 16s 477ms/step - loss: 0.3837 - accuracy: 0.8078 - val_loss: 0.9224 - val_accuracy: 0.7031\n",
      "Epoch 33/50\n",
      "34/34 [==============================] - 16s 474ms/step - loss: 0.3837 - accuracy: 0.8355 - val_loss: 0.8915 - val_accuracy: 0.7109\n",
      "Epoch 34/50\n",
      "34/34 [==============================] - 16s 467ms/step - loss: 0.3803 - accuracy: 0.8226 - val_loss: 0.7111 - val_accuracy: 0.7109\n",
      "Epoch 35/50\n",
      "34/34 [==============================] - 16s 486ms/step - loss: 0.3756 - accuracy: 0.8318 - val_loss: 0.7903 - val_accuracy: 0.6953\n",
      "Epoch 36/50\n",
      "34/34 [==============================] - 17s 507ms/step - loss: 0.3505 - accuracy: 0.8540 - val_loss: 0.8484 - val_accuracy: 0.6562\n",
      "Epoch 37/50\n",
      "34/34 [==============================] - 16s 474ms/step - loss: 0.3497 - accuracy: 0.8503 - val_loss: 1.0093 - val_accuracy: 0.6875\n",
      "Epoch 38/50\n",
      "34/34 [==============================] - 16s 473ms/step - loss: 0.3794 - accuracy: 0.8318 - val_loss: 0.9012 - val_accuracy: 0.6641\n",
      "Epoch 39/50\n",
      "34/34 [==============================] - 16s 480ms/step - loss: 0.3052 - accuracy: 0.8614 - val_loss: 0.8402 - val_accuracy: 0.7188\n",
      "Epoch 40/50\n",
      "34/34 [==============================] - 16s 467ms/step - loss: 0.3203 - accuracy: 0.8336 - val_loss: 0.9428 - val_accuracy: 0.7031\n",
      "Epoch 41/50\n",
      "34/34 [==============================] - 16s 468ms/step - loss: 0.3141 - accuracy: 0.8688 - val_loss: 0.9405 - val_accuracy: 0.6719\n",
      "Epoch 42/50\n",
      "34/34 [==============================] - 16s 469ms/step - loss: 0.3192 - accuracy: 0.8429 - val_loss: 0.9327 - val_accuracy: 0.7188\n",
      "Epoch 43/50\n",
      "34/34 [==============================] - 16s 483ms/step - loss: 0.2993 - accuracy: 0.8669 - val_loss: 1.1195 - val_accuracy: 0.6250\n",
      "Epoch 44/50\n",
      "34/34 [==============================] - 16s 473ms/step - loss: 0.3080 - accuracy: 0.8725 - val_loss: 0.8373 - val_accuracy: 0.6719\n",
      "Epoch 45/50\n",
      "34/34 [==============================] - 16s 470ms/step - loss: 0.2826 - accuracy: 0.8946 - val_loss: 0.9003 - val_accuracy: 0.6875\n",
      "Epoch 46/50\n",
      "34/34 [==============================] - 16s 466ms/step - loss: 0.2180 - accuracy: 0.9076 - val_loss: 1.3141 - val_accuracy: 0.7266\n",
      "Epoch 47/50\n",
      "34/34 [==============================] - 16s 484ms/step - loss: 0.3163 - accuracy: 0.8725 - val_loss: 1.0714 - val_accuracy: 0.6641\n",
      "Epoch 48/50\n",
      "34/34 [==============================] - 16s 471ms/step - loss: 0.2456 - accuracy: 0.8983 - val_loss: 1.2732 - val_accuracy: 0.7031\n",
      "Epoch 49/50\n",
      "34/34 [==============================] - 16s 474ms/step - loss: 0.2763 - accuracy: 0.8688 - val_loss: 1.3177 - val_accuracy: 0.6719\n",
      "Epoch 50/50\n",
      "34/34 [==============================] - 16s 484ms/step - loss: 0.2607 - accuracy: 0.8965 - val_loss: 0.9209 - val_accuracy: 0.7031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f44e0a0a30>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=557//batch_size,epochs=50,\n",
    "                    validation_data=test_generator,\n",
    "                    validation_steps=140//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33297cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('DogsVsCats.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1c1a3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "937e83a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmodel=load_model('DogsVsCats.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bbefb7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=image.load_img('cat1.jpg',target_size=(150,150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "083329cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array=image.img_to_array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d1b32142",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array=img_array.reshape((1,)+img_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "55725172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction=nmodel.predict(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "16bd9fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the image is a cat\n"
     ]
    }
   ],
   "source": [
    "if prediction[0]<0.5:\n",
    "    print('the image is a cat')\n",
    "elif prediction[0]>0.5:\n",
    "    print('the image is a dog')\n",
    "else:\n",
    "    print('image is neither dog or cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd402b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6bad6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
